{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Medical_segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalGrzebyk/Medical_segmentation/blob/master/Medical_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smAIY0NGCS-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Maul0i7WHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing directories for .png files\n",
        "!mkdir /content/train\n",
        "!mkdir /content/train/data\n",
        "!mkdir /content/train/data/1\n",
        "!mkdir /content/train/mask\n",
        "!mkdir /content/train/mask/1\n",
        "!mkdir /content/val\n",
        "!mkdir /content/val/data\n",
        "!mkdir /content/val/data/1\n",
        "!mkdir /content/val/mask\n",
        "!mkdir /content/val/mask/1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHzK4N1iBV0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade nibabel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6bPJrEiBvCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget \"DATASET LINK\"  -O public.zip\n",
        "#!unzip -q public.zip\n",
        "#!rm public.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_u1Hc4bEQRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install segmentation_models\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zlib\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from segmentation_models.losses import dice_loss\n",
        "from segmentation_models import Unet\n",
        "from segmentation_models import get_preprocessing\n",
        "from segmentation_models.metrics import iou_score, f1_score\n",
        "from typing import Tuple, List\n",
        "from pathlib import Path\n",
        "import cv2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJsfv3uBFXAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_raw_volume(path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    data: nib.Nifti1Image = nib.load(str(path))\n",
        "    data = nib.as_closest_canonical(data)\n",
        "    raw_data = data.get_fdata(caching='unchanged', dtype=np.float32)\n",
        "    return raw_data, data.affine\n",
        "\n",
        "\n",
        "def load_labels_volume(path: Path) -> np.ndarray:\n",
        "    return load_raw_volume(path)[0].astype(np.uint8)\n",
        "\n",
        "\n",
        "def save_labels(data: np.ndarray, affine: np.ndarray, path: Path):\n",
        "    nib.save(nib.Nifti1Image(data, affine), str(path))\n",
        "\n",
        "\n",
        "def show_slices(slices: List[np.ndarray]):\n",
        "    fig, axes = plt.subplots(1, len(slices))\n",
        "    for i, data_slice in enumerate(slices):\n",
        "        axes[i].imshow(data_slice.T, cmap=\"gray\", origin=\"lower\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yy42fFmSeg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting .png slices from 3D scans (there was two different datasets)\n",
        "def predata():\n",
        "    inputs = ['FirstDataset/train/', 'SecondDataset/train/']\n",
        "    outputs = ['/content/train/data/1/', '/content/train/mask/1/', '/content/val/data/1/', '/content/val/mask/1/']\n",
        "    data_path = [name for name in sorted(Path(inputs[1]).iterdir())]\n",
        "    size = len(data_path)\n",
        "    for num, file in enumerate(data_path):\n",
        "        data_path2 = [name_ for name_ in sorted(Path(file).iterdir())]\n",
        "        d_p = data_path2[0]\n",
        "        m_p = data_path2[1]\n",
        "        tmp_img, aff = load_raw_volume(d_p)\n",
        "        tmp_musk = load_labels_volume(m_p)\n",
        "        x_size, y_size, z_size =tmp_img.shape\n",
        "        for y_index in range(y_size):\n",
        "            data_slice = tmp_img[:, y_index]\n",
        "            data_slice = cv2.resize(data_slice, (256, 256))\n",
        "            mask_slice = tmp_musk[:, y_index]\n",
        "            mask_slice = cv2.resize(mask_slice, (256, 256))\n",
        "            if num / size < 0.9:\n",
        "                name = outputs[0] + 'x%04d%04d.png' % (num, y_index)\n",
        "                name_mask = outputs[1] + 'x%04d%04d.png' % (num, y_index)\n",
        "            else:\n",
        "                name = outputs[2] + 'x%04d%04d.png' % (num, y_index)\n",
        "                name_mask = outputs[3] + 'x%04d%04d.png' % (num, y_index)\n",
        "            plt.imsave(name, data_slice, format='png', cmap='gray', origin='lower')\n",
        "            plt.imsave(name_mask, mask_slice, format='png', cmap='gray', origin='lower')\n",
        "\n",
        "    data_path = [name for name in sorted(Path(inputs[0]).iterdir()) if not name.name.endswith('mask.nii.gz')]\n",
        "    size = len(data_path)\n",
        "    for num, file in enumerate(data_path):\n",
        "        tmp_img, aff = load_raw_volume(file)\n",
        "        tmp_musk = load_labels_volume(str(file).replace(\".nii.gz\", \"_mask.nii.gz\"))\n",
        "        x_size, y_size, z_size =tmp_img.shape\n",
        "        for y_index in range(y_size):\n",
        "            data_slice = tmp_img[:, y_index]\n",
        "            data_slice = cv2.resize(data_slice, (256, 256))\n",
        "            mask_slice = tmp_musk[:, y_index]\n",
        "            mask_slice = cv2.resize(mask_slice, (256, 256))\n",
        "            if num / size < 0.9:\n",
        "                name = outputs[0] + 'x%04d%04d.png' % (num, y_index)\n",
        "                name_mask = outputs[1] + 'x%04d%04d.png' % (num, y_index)\n",
        "            else:\n",
        "                name = outputs[2] + 'x%04d%04d.png' % (num, y_index)\n",
        "                name_mask = outputs[3] + 'x%04d%04d.png' % (num, y_index)\n",
        "            plt.imsave(name, data_slice, format='png', cmap='gray', origin='lower')\n",
        "            plt.imsave(name_mask, mask_slice, format='png', cmap='gray', origin='lower')\n",
        "predata()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_aJUtoS18e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def from_directory_datagen():\n",
        "    flow_params = {'target_size': (256, 256),\n",
        "                   'class_mode': None,\n",
        "                   'color_mode': 'rgb'\n",
        "                   }\n",
        "\n",
        "    images_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1 / 255,\n",
        "    )\n",
        "\n",
        "    mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1 / 255,\n",
        "    )\n",
        "\n",
        "    tr_im = images_datagen.flow_from_directory(\n",
        "        '/content/train/data/',\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "        **flow_params\n",
        "    )\n",
        "\n",
        "    tr_mask = mask_datagen.flow_from_directory(\n",
        "        '/content/train/mask/',\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "        **flow_params\n",
        "    )\n",
        "    val_im = images_datagen.flow_from_directory(\n",
        "        '/content/val/data/',\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "        **flow_params\n",
        "    )\n",
        "\n",
        "    val_mask = mask_datagen.flow_from_directory(\n",
        "        '/content/val/mask/',\n",
        "        batch_size=32,\n",
        "        seed=42,\n",
        "        **flow_params\n",
        "    )\n",
        "\n",
        "    return tr_im, tr_mask, val_im, val_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZA7wO2NS4Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_net():\n",
        "  with tf.device(\"/gpu:0\"):\n",
        "    backbone = 'resnet50'\n",
        "    preprocess_input = get_preprocessing(backbone)\n",
        "\n",
        "    # load your data\n",
        "    x_train, y_train, x_val, y_val = from_directory_datagen()\n",
        "\n",
        "    # preprocess input\n",
        "    x_train = preprocess_input(x_train)\n",
        "    x_val = preprocess_input(x_val)\n",
        "\n",
        "    # define model\n",
        "    model = Unet(backbone, encoder_weights='imagenet', input_shape=(256, 256, 3))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss=dice_loss,\n",
        "                  metrics=[f1_score, iou_score])\n",
        "    \n",
        "    check_point = [ModelCheckpoint('/content/drive/My Drive/model-{epoch:03d}-{val_f1-score:03f}.h5', verbose=1,\n",
        "                             monitor='val_f1-score',\n",
        "                             save_best_only=True, mode='max')]\n",
        "\n",
        "    # fit model\n",
        "    model.fit(\n",
        "        x=(pair for pair in zip(x_train, y_train)),\n",
        "        epochs=10,\n",
        "        steps_per_epoch=x_train.n // x_train.batch_size,\n",
        "        validation_data=(pair for pair in zip(x_val, y_val)),\n",
        "        validation_steps=x_val.n // x_val.batch_size,\n",
        "        verbose=1,\n",
        "        shuffle=True,\n",
        "        callbacks=check_point,\n",
        "    )\n",
        "    model.save('/content/drive/My Drive/unet2.h5')\n",
        "train_net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObtbRfOuRxJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_3D():\n",
        "    first_dataset_path = Path('FirstDataset/test')\n",
        "    second_dataset_path = Path('SecondDataset/test')\n",
        "\n",
        "    backbone = 'resnet50'\n",
        "    preprocess_input = get_preprocessing(backbone)\n",
        "\n",
        "    # define model\n",
        "    model = Unet(backbone, encoder_weights='imagenet', input_shape=(None, None, 3))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss=dice_loss,\n",
        "                  metrics=[f1_score, iou_score])\n",
        "    model.load_weights('/content/drive/My Drive/model-final.h5')\n",
        "\n",
        "    for scan_path in first_dataset_path.iterdir():\n",
        "        if scan_path.name.endswith('mask.nii.gz'):\n",
        "            print(nib.load(str(scan_path)).header.get_zooms())\n",
        "\n",
        "    print()\n",
        "\n",
        "    for scan_path in second_dataset_path.iterdir():\n",
        "        print(nib.load(str(scan_path / 'T1w.nii.gz')).header.get_zooms())\n",
        "\n",
        "    predictions_base_path = Path('/content/drive/My Drive/Predictions')\n",
        "    first_dataset_predictions_path = predictions_base_path / 'first'\n",
        "    second_dataset_predictions_path = predictions_base_path / 'second'\n",
        "\n",
        "    first_dataset_predictions_path.mkdir(exist_ok=True, parents=True)\n",
        "    second_dataset_predictions_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    first_dataset_test_path = Path('FirstDataset/test')\n",
        "    second_dataset_test_path = Path('SecondDataset/test')\n",
        "    fit = MinMaxScaler()\n",
        "\n",
        "    for scan_path in first_dataset_test_path.iterdir():\n",
        "        data, affine = load_raw_volume(scan_path)\n",
        "        labels = np.zeros(data.shape, dtype=np.uint8)\n",
        "\n",
        "        x_size, y_size, z_size = data.shape\n",
        "        for y_index in range(y_size):\n",
        "            data_slice = data[:, y_index, :]\n",
        "            data_slice = cv2.resize(data_slice, (256, 256))\n",
        "            data_slice = fit.fit_transform(data_slice)\n",
        "            data_slice = cv2.cvtColor(data_slice, cv2.COLOR_GRAY2RGB)\n",
        "            prediction = model.predict(data_slice[None, :])\n",
        "            prediction[prediction < 0.5] = 0\n",
        "            prediction[prediction >= 0.5] = 1\n",
        "            prediction = prediction.squeeze()\n",
        "            labels[:, y_index, :] = cv2.resize(prediction, (z_size, x_size))\n",
        "\n",
        "        save_labels(labels, affine, first_dataset_predictions_path / scan_path.name)\n",
        "\n",
        "    for scan_path in second_dataset_test_path.iterdir():\n",
        "        data, affine = load_raw_volume(scan_path / 'T1w.nii.gz')\n",
        "        labels = np.zeros(data.shape, dtype=np.uint8)\n",
        "\n",
        "        x_size, y_size, z_size = data.shape\n",
        "        for y_index in range(y_size):\n",
        "            data_slice = data[:, y_index, :]\n",
        "            data_slice = cv2.resize(data_slice, (256, 256))\n",
        "            data_slice = fit.fit_transform(data_slice)\n",
        "            data_slice = cv2.cvtColor(data_slice, cv2.COLOR_GRAY2RGB)\n",
        "            prediction = model.predict(data_slice[None, :])\n",
        "            prediction[prediction < 0.5] = 0\n",
        "            prediction[prediction >= 0.5] = 1\n",
        "            prediction = prediction.squeeze()\n",
        "            labels[:, y_index, :] = cv2.resize(prediction, (z_size, x_size))\n",
        "        save_labels(labels, affine, second_dataset_predictions_path / f'{scan_path.name}.nii.gz')\n",
        "\n",
        "predict_3D()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0DikBmtVC1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_3D():\n",
        "    mean = 0\n",
        "    i = 0\n",
        "    first_dataset_predictions_path = Path('/content/drive/My Drive/Predictions/first')\n",
        "    second_dataset_predictions_path = Path('/content/drive/My Drive/Predictions/second')\n",
        "    for dataset_predictions_path in (first_dataset_predictions_path, second_dataset_predictions_path):\n",
        "        for prediction_path in dataset_predictions_path.iterdir():\n",
        "            prediction_name = prediction_path.name[:-7]  # deleting '.nii.gz' from filename\n",
        "            prediction = nib.load(str(prediction_path))\n",
        "\n",
        "            response = requests.post(f'link to prediction checker{prediction_name}',\n",
        "                                     data=zlib.compress(prediction.to_bytes()))\n",
        "            if response.status_code == 200:\n",
        "                print(dataset_predictions_path.name, prediction_path.name, response.json())\n",
        "                mean += response.json()['dice']\n",
        "                i += 1\n",
        "            else:\n",
        "                print(f'Error processing prediction {dataset_predictions_path.name}/{prediction_name}: {response.text}')\n",
        "\n",
        "    print(mean/i)\n",
        "check_3D()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}